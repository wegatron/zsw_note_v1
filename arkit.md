---
tag: programming/tools
---
## Target
1. æŠ åƒ, occlusion
    __front camera:__
    [Using AVFoundationâ€™s AVCapturePhotoOutput, your app can find out whether a particular camera configuration supports the delivery of a portrait effect mattes to still images, and opt in to have them delivered on a per-photo request basis. AVCapturePhotoOutput delivers photo results using an in-memory wrapper object called AVCapturePhoto.](https://developer.apple.com/documentation/avfoundation/cameras_and_media_capture/enhancing_live_video_by_leveraging_truedepth_camera_data)

    __ğŸrear camera with lidar:__
    [use segmentationBuffer and estimatedDepthData to implement people occlusion yourself.](https://developer.apple.com/documentation/arkit/arconfiguration/3089121-framesemantics)
 
2. å°†äººç‰©ç§»åŠ¨åˆ°ç›¸åŒåœºæ™¯çš„åŒä¸€ä¸ªä½ç½®.
    [ARBodyAnchor](https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration) + åœºæ™¯scanning

3. è‡ªæ‹æ•ˆæœï¼Œæ‚¨å¯ä»¥åœ¨è„¸éƒ¨ç½‘æ ¼ä¸Šæ¸²æŸ“åŠé€æ˜çš„çº¹ç†ï¼Œä»¥å®ç°è™šæ‹Ÿçº¹èº«æˆ–è„¸éƒ¨æ¶‚æ–™ç­‰æ•ˆæœï¼Œæˆ–è€…è¿›è¡ŒåŒ–å¦†ï¼Œç•™èƒ¡é¡»æˆ–èƒ¡é¡»ï¼Œæˆ–è€…ç”¨ç å®ï¼Œé¢å…·é®ç›–ç½‘æ ¼ï¼Œ å¸½å­å’Œçœ¼é•œã€‚

4. é¢éƒ¨æ•æ‰ï¼Œæ‚¨å¯ä»¥å®æ—¶æ•æ‰é¢éƒ¨è¡¨æƒ…å¹¶å°†å…¶ç”¨ä½œå°†è¡¨æƒ…æŠ•å½±åˆ°è™šæ‹Ÿè§’è‰²æˆ–æ¸¸æˆä¸­è§’è‰²çš„è£…å¤‡ã€‚


## Apple Arkit
1. ğŸ‹scanning & reconstruction (point cloud, mesh)
    demo
2. ğŸ‹Location anchor
3. sematic information
    * ğŸ‹classification
    * motion capture
    * tracking
    * ğŸ‹people occlution
    >We have a new property on ARConfiguration called FrameSemantics.
    This will give you different kinds of semantic information of what's in the current frame.
    You can also check if a certain semantic is available on the specific configuration or device with an additional method on the ARConfiguration.
    Specific for people occlusion, there are two methods available that you can use. One option is person segmentation.
    This will-- you provide just with the segmentation of people rendered on top of the camera image.
    That's the best choice if you know that people will always be standing upfront and your virtual content will always be behind those people.

### Framework & API Reference
* [ARConfiguration](https://developer.apple.com/documentation/arkit/arconfiguration)
    ä¸åŒçš„ä¸šåŠ¡éœ€æ±‚ä½¿ç”¨ä¸åŒçš„configurationå®ä½“(algorithm flow), å¹¶å¯ä»¥è¿›è¡Œä¸€äº›åŠŸèƒ½æ˜¯å¦å¯ç”¨çš„é…ç½®.
* [ARSession](https://developer.apple.com/documentation/arkit/arsession)
    ç®—æ³•æµçš„æ„å»º, å¹¶ç®¡ç†ç¡¬ä»¶ä»¥åŠç®—æ³•æµç¨‹çš„(SLAM)çŠ¶æ€:
    1. ç®¡ç†ç¡¬ä»¶(camera, gps, lidar, gryo)
    2. æ ¹æ®configuration, ç»„å»ºå¹¶ç®¡ç†ç®—æ³•æµç¨‹
* [ARAnchor](https://developer.apple.com/documentation/arkit/aranchor)

* [ARFrame](https://developer.apple.com/documentation/arkit/arframe)
    ARFrameè·å–çš„ä¸¤ç§æ–¹å¼: æ¨é€å¼(ä¸ºARSessionæ³¨å†Œ`delegate`), ä¸»åŠ¨è·å–`currentFrame`.

## Apple Realitykit
RealityKit provides photo-realistic rendering, camera effects, animations, physics, and a lot more. It was built from the ground up specifically for AR.

1. physics interaction between virtual objects and real ones
2. rendering
    * occlusion
    * real and virtual illumination both real and unreal object
    >And ARKit uses your face as a light probe to estimate lighting conditions, and generates spherical harmonics coefficients that you can apply to your rendering.
3. Face tracking, 

## å‚æ•°
Lidar 60HZ
Front depth image 15HZ

## ARç›¸å…³çš„ä¿¡æ¯ã€èµ„æ–™
* [Spark AR Studio](https://sparkar.facebook.com/ar-studio/)
    face bookçš„AR æ»¤é•œåˆ¶ä½œ/è®¾è®¡å·¥å…·(æ— éœ€å†™ä»£ç )

* [ARCore Depth API1](https://developers.googleblog.com/2020/06/a-new-wave-of-ar-realism-with-arcore-depth-api.html)

* [ARCore Depth API2](https://www.infoq.com/news/2020/06/ARCore-depth-api-released/)
    ä¸¤ä¸ªproject: [lines-of-play](https://github.com/googlecreativelab/lines-of-play), [arcore-depth-lab](https://github.com/googlesamples/arcore-depth-lab/)


## å…¶ä»–
åŒæ‘„æ·±åº¦å›¾åˆ¤æ–­:
```c++
AVCapturePhotoOutput* photoOutput = [[AVCapturePhotoOutput alloc] init];
 if ([photoOutput isDepthDataDeliverySupported]) {
            
       }
```

## Reference
ğŸ…[Explore Arkit4 Notes](https://www.wwdcnotes.com/notes/wwdc20/10611/)
[app galary](https://github.com/olucurious/Awesome-ARKit)